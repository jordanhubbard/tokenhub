# Tokenhub Examples

This file contains examples of how to use the Tokenhub service.

## Setup

1. Generate an encryption key:
```bash
go run cmd/tokenhub/main.go -generate-key
```

2. Set environment variables:
```bash
export TOKENHUB_ENCRYPTION_KEY=<your-key>
export OPENAI_API_KEY=<your-openai-key>
export ANTHROPIC_API_KEY=<your-anthropic-key>
```

3. Start the service:
```bash
# With default config
go run cmd/tokenhub/main.go

# With custom config
go run cmd/tokenhub/main.go -config config.json

# With Docker
docker run -p 8080:8080 \
  -e TOKENHUB_ENCRYPTION_KEY=$TOKENHUB_ENCRYPTION_KEY \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  -e ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \
  tokenhub
```

## API Examples

### Chat Completions

Basic chat request:
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Explain quantum computing in simple terms"}
    ],
    "max_tokens": 500,
    "temperature": 0.7
  }'
```

Multi-turn conversation:
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "system", "content": "You are a helpful coding assistant"},
      {"role": "user", "content": "How do I implement a binary search in Go?"},
      {"role": "assistant", "content": "Here is a binary search implementation..."},
      {"role": "user", "content": "Can you add error handling?"}
    ],
    "max_tokens": 1000,
    "temperature": 0.5
  }'
```

### Text Completions

```bash
curl -X POST http://localhost:8080/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Once upon a time in a land far away",
    "max_tokens": 200,
    "temperature": 0.8
  }'
```

### Adversarial Orchestration

This feature uses two models - one to generate a plan and another to critique it:

```bash
curl -X POST http://localhost:8080/v1/adversarial \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Design a microservices architecture for an e-commerce platform"
  }'
```

Response will include:
- `original_prompt`: Your input prompt
- `initial_plan`: Plan generated by Model A
- `critique`: Critique from Model B
- `refined_plan`: Refined plan from Model A based on critique
- `total_tokens`: Total tokens used across all phases

Example response:
```json
{
  "original_prompt": "Design a microservices architecture for an e-commerce platform",
  "initial_plan": "1. User Service\n2. Product Service\n3. Order Service...",
  "critique": "The plan is solid but missing considerations for...",
  "refined_plan": "Based on the critique, here's an improved architecture...",
  "total_tokens": 2450
}
```

### Health Check

```bash
curl http://localhost:8080/health
```

Response:
```json
{
  "status": "healthy"
}
```

## Configuration Examples

### Basic Configuration

Create `config.json`:
```json
{
  "server": {
    "host": "0.0.0.0",
    "port": 8080
  },
  "vault": {
    "encryption_key_env": "TOKENHUB_ENCRYPTION_KEY"
  },
  "providers": [
    {
      "name": "openai",
      "type": "openai",
      "api_key_env": "OPENAI_API_KEY"
    }
  ],
  "models": [
    {
      "id": "gpt-3.5-turbo",
      "provider": "openai",
      "name": "gpt-3.5-turbo",
      "weight": 80,
      "cost_per_1k": 0.002,
      "context_size": 4096,
      "capabilities": ["chat", "completion"]
    }
  ]
}
```

### Multi-Provider Configuration

```json
{
  "server": {
    "host": "0.0.0.0",
    "port": 8080
  },
  "vault": {
    "encryption_key_env": "TOKENHUB_ENCRYPTION_KEY"
  },
  "providers": [
    {
      "name": "openai",
      "type": "openai",
      "api_key_env": "OPENAI_API_KEY"
    },
    {
      "name": "anthropic",
      "type": "anthropic",
      "api_key_env": "ANTHROPIC_API_KEY"
    },
    {
      "name": "vllm-local",
      "type": "vllm",
      "base_url": "http://localhost:8000"
    }
  ],
  "models": [
    {
      "id": "gpt-4",
      "provider": "openai",
      "name": "gpt-4",
      "weight": 90,
      "cost_per_1k": 0.03,
      "context_size": 8192,
      "capabilities": ["chat", "completion"]
    },
    {
      "id": "claude-2",
      "provider": "anthropic",
      "name": "claude-2",
      "weight": 85,
      "cost_per_1k": 0.01,
      "context_size": 100000,
      "capabilities": ["chat", "completion"]
    },
    {
      "id": "llama-local",
      "provider": "vllm-local",
      "name": "meta-llama/Llama-2-70b-chat-hf",
      "weight": 70,
      "cost_per_1k": 0.0,
      "context_size": 4096,
      "capabilities": ["chat", "completion"]
    }
  ]
}
```

## Automatic Escalation

Tokenhub automatically handles failures and context overflows:

### Context Overflow
If a request exceeds the context size of the selected model, Tokenhub automatically escalates to a model with a larger context window.

### Provider Failure
If a provider fails, Tokenhub automatically tries alternative providers based on model weights and capabilities.

Example: If OpenAI is down, a request will automatically be routed to Anthropic or a vLLM instance.

## Model Selection

Models are selected based on:
1. **Context Size**: Must support the required context
2. **Cost**: Lower cost is preferred (if max_cost is set)
3. **Weight**: Higher weight indicates higher priority

Selection formula: `score = weight - (cost_per_1k * 10)`

## Docker Compose Example

Create `docker-compose.yml`:
```yaml
version: '3.8'

services:
  tokenhub:
    build: .
    ports:
      - "8080:8080"
    environment:
      - TOKENHUB_ENCRYPTION_KEY=${TOKENHUB_ENCRYPTION_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    volumes:
      - ./config.json:/config.json:ro
    command: ["/tokenhub", "-config", "/config.json"]
    restart: unless-stopped
```

Run with:
```bash
docker-compose up -d
```

## Security Best Practices

1. **Never commit API keys** to version control
2. **Use environment variables** for sensitive data
3. **Rotate encryption keys** periodically
4. **Use HTTPS** in production
5. **Limit access** with firewall rules or API gateway
6. **Monitor logs** for suspicious activity

## Troubleshooting

### Service won't start
- Check if port 8080 is available
- Verify environment variables are set
- Check logs for error messages

### Provider errors
- Verify API keys are correct
- Check provider API status
- Ensure network connectivity

### Context overflow not working
- Verify multiple models are configured
- Check that larger models have higher context_size values
- Review logs for escalation attempts
