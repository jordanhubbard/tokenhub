# Tokenhub

Organize your token providers by cost, complexity, and reliability

## Overview

Tokenhub is an LLM interposer service that intelligently routes requests between multiple LLM providers (OpenAI, Anthropic, local vLLM instances) based on cost, context size, and availability. It provides advanced features like automatic escalation on failures, context overflow handling, and adversarial orchestration mode.

## Features

- **Provider Registry**: Support for OpenAI, Anthropic, and vLLM providers
- **API Key Management**: Secure storage with AES-256 encryption via encrypted vault or environment variables
- **Model Registry**: Configure models with weight, cost, and context size metadata
- **Intelligent Routing**: Automatic model selection based on requirements
- **Automatic Escalation**: Fallback to alternative providers on failure or context overflow
- **Adversarial Orchestration**: Model A generates plan, Model B critiques, Model A refines
- **Containerized**: Docker support for easy deployment

## Quick Start

### Generate Encryption Key

```bash
go run cmd/tokenhub/main.go -generate-key
```

Set the generated key as an environment variable:
```bash
export TOKENHUB_ENCRYPTION_KEY=<generated-key>
```

### Configuration

Copy the example configuration:
```bash
cp config.example.json config.json
```

Edit `config.json` to add your providers and models. API keys can be provided via environment variables (recommended) or directly in the config.

### Running with Docker

```bash
# Build the image
docker build -t tokenhub .

# Run with environment variables
docker run -p 8080:8080 \
  -e TOKENHUB_ENCRYPTION_KEY=$TOKENHUB_ENCRYPTION_KEY \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  -e ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \
  tokenhub
```

Or use docker-compose:
```bash
docker-compose up
```

### Running Locally

```bash
# Install dependencies
go mod download

# Run the service
go run cmd/tokenhub/main.go -config config.json
```

## API Endpoints

### Chat Completions

```bash
POST /v1/chat/completions
Content-Type: application/json

{
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "max_tokens": 100,
  "temperature": 0.7
}
```

### Completions

```bash
POST /v1/completions
Content-Type: application/json

{
  "prompt": "Once upon a time",
  "max_tokens": 100,
  "temperature": 0.7
}
```

### Adversarial Orchestration

```bash
POST /v1/adversarial
Content-Type: application/json

{
  "prompt": "Design a microservices architecture for an e-commerce platform"
}
```

Response includes:
- `initial_plan`: Plan generated by Model A
- `critique`: Critique from Model B
- `refined_plan`: Refined plan from Model A

### Health Check

```bash
GET /health
```

## Configuration

The configuration file supports:

- **Server**: Host and port settings
- **Vault**: Encryption key configuration
- **Providers**: List of LLM providers with API keys
- **Models**: Model definitions with weights, costs, and context sizes

See `config.example.json` for a complete example.

## Architecture

```
┌─────────────┐
│   Client    │
└──────┬──────┘
       │
       v
┌─────────────────────────────────────┐
│         HTTP Server                  │
│  /v1/chat/completions                │
│  /v1/completions                     │
│  /v1/adversarial                     │
└──────┬──────────────────────────────┘
       │
       v
┌─────────────────────────────────────┐
│         Router                       │
│  - Model Selection                   │
│  - Escalation Logic                  │
│  - Context Overflow Handling         │
└──────┬──────────────────────────────┘
       │
       v
┌─────────────────────────────────────┐
│    Provider Registry                 │
│  - OpenAI                            │
│  - Anthropic                         │
│  - vLLM                              │
└─────────────────────────────────────┘
```

## Security

- API keys are stored encrypted using AES-256 with GCM mode
- Encryption key should be stored securely (e.g., environment variable, secrets manager)
- Never commit API keys or encryption keys to version control

## Development

```bash
# Run tests
go test ./...

# Build
go build -o tokenhub ./cmd/tokenhub

# Run
./tokenhub -config config.json
```

## License

See LICENSE file for details.
