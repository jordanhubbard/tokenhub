services:
  tokenhub:
    image: tokenhub:v0.2.5-6-g13a5d7a-dirty
    ports:
      - "8090:8080"
    environment:
      - TOKENHUB_LISTEN_ADDR=:8080
      - TOKENHUB_DB_DSN=/data/tokenhub.sqlite
      - TOKENHUB_VAULT_ENABLED=true
      - TOKENHUB_OPENAI_API_KEY=${TOKENHUB_OPENAI_API_KEY}
      - TOKENHUB_ANTHROPIC_API_KEY=${TOKENHUB_ANTHROPIC_API_KEY}
      - TOKENHUB_VLLM_ENDPOINTS=${TOKENHUB_VLLM_ENDPOINTS}
      - TOKENHUB_ADMIN_TOKEN=${TOKENHUB_ADMIN_TOKEN}
      - TOKENHUB_CORS_ORIGINS=${TOKENHUB_CORS_ORIGINS}
      - TOKENHUB_RATE_LIMIT_RPS=${TOKENHUB_RATE_LIMIT_RPS:-60}
      - TOKENHUB_RATE_LIMIT_BURST=${TOKENHUB_RATE_LIMIT_BURST:-120}
      - TOKENHUB_TEMPORAL_ENABLED=${TOKENHUB_TEMPORAL_ENABLED:-false}
      - TOKENHUB_TEMPORAL_HOST=temporal:7233
      - TOKENHUB_TEMPORAL_NAMESPACE=${TOKENHUB_TEMPORAL_NAMESPACE:-tokenhub}
      - TOKENHUB_TEMPORAL_TASK_QUEUE=${TOKENHUB_TEMPORAL_TASK_QUEUE:-tokenhub-tasks}
    volumes:
      - tokenhub_data:/data
    # Temporal is optional; the app handles its absence gracefully.
    # Uncomment the depends_on block below if you enable Temporal.
    # depends_on:
    #   temporal:
    #     condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "2"

  temporal:
    image: temporalio/auto-setup:latest
    ports:
      - "7233:7233"
    environment:
      - DB=sqlite
    volumes:
      - temporal_data:/etc/temporal/data
    healthcheck:
      test: ["CMD", "tctl", "--address", "temporal:7233", "cluster", "health"]
      interval: 15s
      timeout: 5s
      start_period: 30s
      retries: 5
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1"

  temporal-ui:
    image: temporalio/ui:latest
    ports:
      - "8233:8080"
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
    depends_on:
      temporal:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: "0.5"

  # PLACEHOLDER: vLLM service for development only
  # This uses nginx:alpine as a mock service for local development.
  # In production, replace this with an actual vLLM instance running a real language model.
  # For example: image: vllm/vllm-openai:latest
  vllm-1:
    image: nginx:alpine
    ports:
      - "8000:80"
    restart: unless-stopped

volumes:
  tokenhub_data:
  temporal_data:
